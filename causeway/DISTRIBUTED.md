## Distributed Tracing Roadmap

### Goal
Extend Raceway beyond single-process traces so a request flowing across multiple services appears as one causal graph with shared vector clocks, critical path analysis, and race detection.

### Guiding Principles
- Prefer open standards (W3C Trace Context) for headers and context propagation.
- Optimise for greenfield adoption—no backwards-compatibility constraints.
- Make propagation opt-in per SDK while features are under active development.
- Validate behavior under failure (dropped headers, clock skew, retries).

---

## Phase 0 – Discovery & Design

### Phase 0 Outcomes

1. **Requirements workshop**
   - **SDK inventory:** TypeScript (Node/Express, HTTP only), Python (Flask/FastAPI via requests), Go (net/http), Rust (axum/reqwest). No current gRPC or queue integrations; JS auto-tracking exists but stays single-process.
   - **Must-have scenarios for first release:** HTTP REST between services instrumented with the existing SDKs. Nice-to-have (later phases): gRPC (Go/Rust) and async messaging (Kafka/SQS).
   - **Non-goals for initial rollout:** Legacy header compatibility, polyglot mobile/edge SDKs.

2. **Header/protocol decision**
   - Adopt **W3C `traceparent`/`tracestate`** for baseline propagation to interoperate with existing tooling.
   - Introduce a vendor header `raceway-clock` containing a compact, versioned vector clock payload:
     ```text
     raceway-clock: v1;base64url(<zstd-compressed JSON>)
     ```
     JSON schema (`v1`):
     ```json
     {
       "trace_id": "<uuid>",
       "span_id": "<uuid>",
       "parent_span_id": "<uuid|null>",
       "service": "<string>",
       "clock": [["service-A#instance", 42], ["service-B#instance", 7]]
     }
     ```
   - Future-proof via `v<N>` prefix and optional `flags` array for experimental fields.

3. **Graph/engine design**
   - Extend `EventMetadata` with optional `distributed_span_id` and `upstream_span_id`.
   - Vector clock merge: element-wise max keyed by `<service>#<instance>`; discard entries older than configurable TTL (default 15 minutes).
   - Duplicate suppression: treat `(trace_id, span_id, event_id)` as unique; if incoming event already attached, log and skip.
   - External edges: store in new `DistributedEdge { from_span, to_span, link_type }` structure; engine builds edges post-ingestion.

4. **Storage impact review**
   - **Postgres:** add tables
     ```sql
     distributed_spans(trace_id UUID, span_id UUID PRIMARY KEY, service TEXT, instance TEXT, first_event TIMESTAMPTZ);
     distributed_edges(from_span UUID, to_span UUID, link_type TEXT, metadata JSONB);
     ```
     Indexes on `(trace_id, service)` and `(from_span, to_span)`. Backfill strategy: when distributed tracing disabled, tables remain empty; no migration of legacy data required.
   - **Memory backend:** mirror spans/edges with `DashMap<Uuid, DistributedSpan>` and adjacency lists.
   - No additional backfill necessary (greenfield adoption).

5. **UI requirements**
   - **TUI:** add service summary column in trace list, color-coded service lanes in timeline, breadcrumb showing service transitions.
   - **Web:** enhance dependency graph to consume `distributed_edges`, timeline segmented per service, filters for `service`, `operation`, and header-loss warnings.
   - Provide hover tooltips showing vector clock entries and originating service.

6. **Test strategy draft**
   - Expand existing test matrix (see “Testing Plan”) with:
     - Header interoperability suite comparing Raceway headers against W3C conformance examples.
     - Engine property tests verifying idempotency of duplicate span ingestion.
     - UI screenshot/visual regression tests for multi-service timelines.
   - Plan to introduce `./examples/distributed-demo` for end-to-end CI validation (Node → Go → Python chain).

**Deliverables produced:** this plan (architecture decisions), header specification (`raceway-clock v1`), storage schema sketch, updated milestone plan (Phase 1–4 unchanged but now informed by decisions).

---

## Phase 1 – SDK Propagation Foundations

### Status: ✅ Phase 1.5 Complete!

**Current Progress:**

**Phase 1: Trace Context Modules - ✅ COMPLETE!**
- ✅ **Trace Context Modules** (ALL SDKs COMPLETE!)
  - TypeScript: Full implementation with 23 passing unit tests ✅
  - Python: Full implementation with 23 passing unit tests ✅ (includes bug fix for span_id parsing)
  - Go: Full implementation with 26 passing unit tests ✅ (includes bug fix for span_id parsing + missing runtime import)
  - Rust: Full implementation with 22 passing unit tests + 1 doc-test ✅ (includes bug fix for span_id parsing)
- ✅ **Testing Infrastructure** (COMPLETE!)
  - ✅ TypeScript: Jest configured, comprehensive test suite (serialization, propagation, multi-hop)
  - ✅ Python: pytest configured, comprehensive test suite (serialization, propagation, multi-hop)
  - ✅ Go: go test configured, comprehensive test suite (serialization, propagation, multi-hop)
  - ✅ Rust: cargo test configured, comprehensive test suite (serialization, propagation, multi-hop)
  - ✅ Test harness extended in `./raceway-dev` to run all SDK tests alongside core tests

**Phase 1.5: SDK Propagation - ✅ COMPLETE!**
- ✅ **Middleware Integration** (all SDKs complete!)
  - TypeScript: Express middleware for inbound/outbound propagation ✅
  - Python: Flask/FastAPI middleware ✅
  - Go: net/http middleware with public getters (ServiceName, InstanceID) ✅
  - Rust: Axum tower::Layer ✅
- ✅ **Cross-Language Demo** (`examples/distributed/`)
  - TypeScript → Python → Go → Rust HTTP chain ✅
  - Bash orchestration (no Docker required) ✅
  - Integration smoke test script with header validation ✅
- ✅ **Demo Applications** (complete on ports 6001-6004)
- ✅ **Integrated into `./raceway-dev` menu** (option 11)

**Next Steps:**
1. ✅ ~~Complete all SDK trace context modules~~ DONE!
2. ✅ ~~Implement middleware integration~~ DONE!
3. ✅ ~~Build `examples/distributed/` smoke test~~ DONE!
4. **Phase 2: Engine changes for cross-service graph edges** ← Next milestone
5. Cross-language integration testing with full graph support

**Key Achievement:** All 4 SDKs now have identical trace context implementations with comprehensive test coverage (95 total tests across all SDKs)!

**Phase 1.5 Goal:** Validate SDK propagation works end-to-end BEFORE requiring backend changes (Phase 2). This allows parallel development and early validation.

### Objectives
- Implement the shared trace-context layer across SDKs using the decisions from Phase 0.
- Ensure every SDK can inject and extract distributed tracing metadata over HTTP calls.
- Provide sample middleware/wrappers so framework integration is straightforward.

### Deliverables
- Document header usage (`traceparent`, `tracestate`, `raceway-clock`) for all SDKs.
- Shared serialization/deserialization libraries in each SDK (TypeScript, Python, Go, Rust).
- Middleware/reference implementations for the primary frameworks per language.
- Automated tests or linters verifying builds (TypeScript `tsc`, Rust `cargo fmt`, Go/Python format checks).
- Sample “Service A → Service B” demos (pending cross-language chain).

### Work Breakdown

1. **Trace Context Modules**
   - Define language-specific structures mirroring the `raceway-clock v1` schema.
   - Implement helper APIs:
     - `TraceContext::from_headers(headers) -> Result<Option<Context>>`
     - `TraceContext::to_headers(&self) -> HeaderMap`
     - `TraceContext::merge(&mut self, incoming: &Context)`
   - Include validation (UUID formats, version prefixes) and error reporting.

2. **Outbound Propagation**
   - **TypeScript**
     - Add fetch/axios interceptors that pull context from AsyncLocalStorage and set headers.
     - Extend Express middleware to initialize context and wrap downstream `RacewayClient`.
   - **Python**
     - Provide `requests` and `httpx` session adapters that inject headers.
     - Flask/FastAPI: route middleware seeds context, utilities for background tasks.
   - **Go**
     - Implement `http.RoundTripper` wrapper plus helper for `http.Client`.
     - Add context helpers (`context.Context`) storing trace/span ids.
   - **Rust**
     - Provide `tower::Layer` for axum/reqwest.
     - Add macros or helpers to wrap `reqwest::Client` requests.

3. **Inbound Extraction**
   - Extend existing middleware for Express, Flask/FastAPI, Gin, and Axum to:
     - Parse headers via `TraceContext::from_headers`.
     - Merge with local context (element-wise max of vector clock).
     - Generate new span ids when missing, mark context as “distributed=false” if no headers.

4. **SDK Surface Area Updates**
   - Add ability to set service name/instance id (if not already) for clock keys.
   - Document new config knobs (e.g., `propagation_enabled`, `propagation_headers_debug`).
   - Provide logging hooks to detect propagation issues (missing header, parse failure).

5. **Demo Applications**
   - For each SDK, create a minimal two-service example:
     - Service A receives a request, calls Service B using the SDK client, both emit events, verify they share the same distributed trace id/span structure.
   - Consolidate demos under `examples/distributed/<language>-chain`.

6. **Phase 1.5: Integration Smoke Test**
   - Build cross-language demo: TypeScript → Python → Go → Rust HTTP chain
   - Validate SDK propagation WITHOUT requiring Phase 2 backend changes
   - Location: `examples/distributed/` with docker-compose orchestration

   **What This Validates:**
   - ✅ Headers propagate correctly across all 4 SDKs
   - ✅ Vector clocks accumulate components from all services
   - ✅ Events from all services share the same `trace_id`
   - ✅ Service metadata correctly identifies each service
   - ✅ Middleware integration works in all frameworks (Express, Flask, net/http, Axum)

   **Known Limitations (Acceptable without Phase 2):**
   - ⚠️ Graph shows 4 disconnected sub-graphs (no cross-service edges yet)
   - ⚠️ Critical path doesn't span services (calculated per-service only)
   - ⚠️ No cross-service race detection (within-service races still work)
   - ⚠️ No distributed span hierarchy (span linkage is local to each service)

   **Test Script:** `examples/distributed/test.sh`
   - Starts all services via docker-compose
   - Makes request through full chain
   - Validates headers, vector clocks, and event grouping
   - Reports success with clear note about Phase 2 requirements

### Testing Plan (Phase 1)

| Layer            | Tests                                                                                                     |
|------------------|-----------------------------------------------------------------------------------------------------------|
| Serialization    | Unit tests for encode/decode of `raceway-clock` (valid data, version mismatch, invalid base64). ✅ DONE (95 tests)  |
| Header handling  | Unit tests ensuring `traceparent`/`tracestate` round-trip; verify vendor header appended without clobbering existing state. ✅ DONE |
| Middleware       | Integration tests spinning up local HTTP servers asserting that contexts propagate end-to-end (Service A -> Service B). ⏳ NEXT |
| Merge semantics  | Property tests (where available) checking element-wise max behavior and TTL trimming. ✅ DONE (in unit tests) |
| Regression       | Ensure single-process traces still behave identically when propagation disabled. ⏳ After middleware |
| Smoke Test       | `examples/distributed/test.sh` - Full TS→Py→Go→Rust chain validates header propagation and event grouping. ⏳ Phase 1.5 |

### Exit Criteria

**Phase 1 (Trace Context Modules) - ✅ COMPLETE**
- ✅ All 4 SDKs have trace context serialization/parsing (95 tests passing)
- ✅ Cross-SDK header compatibility validated via unit tests
- ✅ Vector clock merge semantics tested

**Phase 1.5 (SDK Propagation) - ✅ COMPLETE**
- ✅ All maintained SDKs propagate headers on outbound HTTP requests and consume them on inbound requests
- ✅ Middleware integration complete for Express, Flask, net/http, Axum
- ✅ `examples/distributed/` demo runs successfully (bash orchestration)
- ✅ Smoke test validates: header propagation, vector clock accumulation, trace_id consistency
- ✅ Demo integrated into `./raceway-dev` menu (option 11)
- ✅ Go SDK enhanced with public getters for ServiceName() and InstanceID()

**Phase 1.5 Success Criteria - ✅ ACHIEVED:**
```bash
$ cd examples/distributed && ./test.sh
Starting TypeScript service...
Starting Python service...
Starting Go service...
Starting Rust service...

Waiting for TypeScript (port 6001)... ✓
Waiting for Python (port 6002)... ✓
Waiting for Go (port 6003)... ✓
Waiting for Rust (port 6004)... ✓

✓ All services are healthy!

═══════════════════════════════════════════════════════
  Linear Pattern Test: TS → Python → Go → Rust
═══════════════════════════════════════════════════════

W3C traceparent headers:
  ○ TypeScript (entry point - no incoming headers)
  ✓ Python received traceparent
  ✓ Go received traceparent
  ✓ Rust received traceparent

Raceway vector clock headers:
  ○ TypeScript (entry point - no incoming headers)
  ✓ Python received raceway-clock
  ✓ Go received raceway-clock
  ✓ Rust received raceway-clock

✓ Linear pattern test PASSED
  - Headers propagated across all 4 services
  - All services share the same trace_id

⚠️  Note: Graph will show 4 disconnected sub-graphs
   (cross-service edges require Phase 2)

✓ All tests completed!
```

---

## Phase 2 – Engine & Storage Enhancements

### Engine

- Modify `Event` ingestion to accept optional remote parent references (service, span ID, vector clock).
- Update `CausalGraph`:
  - Allow multiple root nodes per logical trace, merging by trace id.
  - Add support for “external edge” linking events from different services.
  - Handle missing parents gracefully (e.g., dropped headers) with warning edges.

### Vector Clocks

- Extend from per-trace `DashMap<Uuid, u64>` to per-service or per-span component.
- Introduce identifier scheme (service name + instance id) to avoid collisions.
- Keep clocks bounded (evict old components).

### Storage

- Postgres migrations:
  - New table for distributed edges (`distributed_edges`).
  - Indexes for `trace_id` + `service_name`.
  - Optional materialized view for service dependency map.
- Memory backend:
  - Mirror distributed edges in-memory (e.g., `DashMap<(Uuid, Uuid), DistributedEdge>`).

### Routing/Config

- Add server config toggles (`distributed_tracing.enabled`, default false).
- Gracefully reject distributed headers when disabled.

### Testing

- Unit tests for graph merge logic.
- Property tests ensuring vector clock happens-before remains correct after merges.
- Storage tests verifying migrations and retrieval of cross-service data.
- Engine integration test: ingest events from two “services” and ensure graph connectivity.

**Exit criteria:** Core engine can accept distributed events and construct a coherent graph; persisted structures support fetching.

---

## Phase 3 – UI & Analytics

### Terminal UI

- Service-aware trace list (show service count).
- Graph view grouping nodes by service with color coding.
- Critical path spanning services.
- Alert when edges missing (header loss).

### Web UI

- Update service dependency graph to use distributed edges.
- Timeline overlay per service.
- Filters for service/operation.

### API

- `/api/traces/:id` responses include distributed edges and service metadata.
- New endpoints (optional) for service-level metrics (e.g., `/api/services/:name/dependencies`).

### Testing

- Snapshot tests for API payloads.
- Cypress/Playwright flows verifying multi-service trace visualization.
- TUI golden tests (ratatui) verifying layout with distributed data.

**Exit criteria:** Users can inspect multi-service traces end-to-end in both UIs.

---

## Phase 4 – Advanced Propagation & Resilience

- gRPC interceptors for all SDKs.
- Message queue/staged propagation (Kafka headers, etc.).
- Fallback clocks when headers missing (generate synthetic spans, mark as incomplete).
- Replay support: ability to rehydrate distributed traces from storage into engine.
- Telemetry & observability: metrics for header adoption, drop rate, clock conflict warnings.

### Tests

- End-to-end scenario: service A → gRPC → service B → Kafka → service C.
- Chaos tests: randomly drop headers to ensure system degrades gracefully.
- Load tests to measure impact on ingestion and storage.

---

## Testing Plan Summary

| Area                  | Tests                                                                                   |
|-----------------------|-----------------------------------------------------------------------------------------|
| SDKs                  | Unit tests for serialization, integration tests with mock servers, contract tests       |
| Engine/Graph          | Unit & property tests on clock merge, integration test ingesting multi-service events    |
| Storage               | Migration tests, round-trip tests for distributed edges                                 |
| APIs                  | Regression tests verifying JSON structure for distributed traces                        |
| TUI/Web               | Snapshot/golden tests for distributed visualizations                                    |
| End-to-End            | Multi-service demo app executed in CI (docker-compose) verifying trace continuity       |
| Resilience            | Chaos-style tests dropping headers, clock skew simulation                               |

---

## Milestones & Deliverables

1. **M0 (Design sign-off)** – approved spec, storage plan, SDK API changes outlined.
2. **M1 (HTTP propagation)** – SDKs send/receive headers; basic engine support behind a feature flag.
3. **M2 (Engine/storage GA)** – distributed traces persisted; API surfaces cross-service data; tests passing.
4. **M3 (UI support)** – TUI/web show multi-service traces; feature flag default-on for beta users.
5. **M4 (Full release)** – gRPC/queues supported; resilience tooling; documentation and sample apps updated.

---

## Documentation & Rollout Tasks

- Update README and docs with configuration and usage instructions.
- Publish sample multi-service demo (docker-compose).
- Release notes per milestone with migration guidance.
- Provide observability guidance (verify context headers in network probes).
