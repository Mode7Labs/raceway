## Distributed Tracing Roadmap

### Goal
Extend Raceway beyond single-process traces so a request flowing across multiple services appears as one causal graph with shared vector clocks, critical path analysis, and race detection.

### Guiding Principles
- Prefer open standards (W3C Trace Context) for headers and context propagation.
- Optimise for greenfield adoption‚Äîno backwards-compatibility constraints.
- Make propagation opt-in per SDK while features are under active development.
- Validate behavior under failure (dropped headers, clock skew, retries).

---

## Phase 0 ‚Äì Discovery & Design

### Phase 0 Outcomes

1. **Requirements workshop**
   - **SDK inventory:** TypeScript (Node/Express, HTTP only), Python (Flask/FastAPI via requests), Go (net/http), Rust (axum/reqwest). No current gRPC or queue integrations; JS auto-tracking exists but stays single-process.
   - **Must-have scenarios for first release:** HTTP REST between services instrumented with the existing SDKs. Nice-to-have (later phases): gRPC (Go/Rust) and async messaging (Kafka/SQS).
   - **Non-goals for initial rollout:** Legacy header compatibility, polyglot mobile/edge SDKs.

2. **Header/protocol decision**
   - Adopt **W3C `traceparent`/`tracestate`** for baseline propagation to interoperate with existing tooling.
   - Introduce a vendor header `raceway-clock` containing a compact, versioned vector clock payload:
     ```text
     raceway-clock: v1;base64url(<zstd-compressed JSON>)
     ```
     JSON schema (`v1`):
     ```json
     {
       "trace_id": "<uuid>",
       "span_id": "<uuid>",
       "parent_span_id": "<uuid|null>",
       "service": "<string>",
       "clock": [["service-A#instance", 42], ["service-B#instance", 7]]
     }
     ```
   - Future-proof via `v<N>` prefix and optional `flags` array for experimental fields.

3. **Graph/engine design**
   - Extend `EventMetadata` with optional `distributed_span_id` and `upstream_span_id`.
   - Vector clock merge: element-wise max keyed by `<service>#<instance>`; discard entries older than configurable TTL (default 15 minutes).
   - Duplicate suppression: treat `(trace_id, span_id, event_id)` as unique; if incoming event already attached, log and skip.
   - External edges: store in new `DistributedEdge { from_span, to_span, link_type }` structure; engine builds edges post-ingestion.

4. **Storage impact review**
   - **Postgres:** add tables
     ```sql
     distributed_spans(trace_id UUID, span_id UUID PRIMARY KEY, service TEXT, instance TEXT, first_event TIMESTAMPTZ);
     distributed_edges(from_span UUID, to_span UUID, link_type TEXT, metadata JSONB);
     ```
     Indexes on `(trace_id, service)` and `(from_span, to_span)`. Backfill strategy: when distributed tracing disabled, tables remain empty; no migration of legacy data required.
   - **Memory backend:** mirror spans/edges with `DashMap<Uuid, DistributedSpan>` and adjacency lists.
   - No additional backfill necessary (greenfield adoption).

5. **UI requirements**
   - **TUI:** add service summary column in trace list, color-coded service lanes in timeline, breadcrumb showing service transitions.
   - **Web:** enhance dependency graph to consume `distributed_edges`, timeline segmented per service, filters for `service`, `operation`, and header-loss warnings.
   - Provide hover tooltips showing vector clock entries and originating service.

6. **Test strategy draft**
   - Expand existing test matrix (see ‚ÄúTesting Plan‚Äù) with:
     - Header interoperability suite comparing Raceway headers against W3C conformance examples.
     - Engine property tests verifying idempotency of duplicate span ingestion.
     - UI screenshot/visual regression tests for multi-service timelines.
   - Plan to introduce `./examples/distributed-demo` for end-to-end CI validation (Node ‚Üí Go ‚Üí Python chain).

**Deliverables produced:** this plan (architecture decisions), header specification (`raceway-clock v1`), storage schema sketch, updated milestone plan (Phase 1‚Äì4 unchanged but now informed by decisions).

---

## Phase 1 ‚Äì SDK Propagation Foundations

### Status: ‚úÖ Phase 1.5 Complete!

**Current Progress:**

**Phase 1: Trace Context Modules - ‚úÖ COMPLETE!**
- ‚úÖ **Trace Context Modules** (ALL SDKs COMPLETE!)
  - TypeScript: Full implementation with 23 passing unit tests ‚úÖ
  - Python: Full implementation with 23 passing unit tests ‚úÖ (includes bug fix for span_id parsing)
  - Go: Full implementation with 26 passing unit tests ‚úÖ (includes bug fix for span_id parsing + missing runtime import)
  - Rust: Full implementation with 22 passing unit tests + 1 doc-test ‚úÖ (includes bug fix for span_id parsing)
- ‚úÖ **Testing Infrastructure** (COMPLETE!)
  - ‚úÖ TypeScript: Jest configured, comprehensive test suite (serialization, propagation, multi-hop)
  - ‚úÖ Python: pytest configured, comprehensive test suite (serialization, propagation, multi-hop)
  - ‚úÖ Go: go test configured, comprehensive test suite (serialization, propagation, multi-hop)
  - ‚úÖ Rust: cargo test configured, comprehensive test suite (serialization, propagation, multi-hop)
  - ‚úÖ Test harness extended in `./raceway-dev` to run all SDK tests alongside core tests

**Phase 1.5: SDK Propagation - ‚úÖ COMPLETE!**
- ‚úÖ **Middleware Integration** (all SDKs complete!)
  - TypeScript: Express middleware for inbound/outbound propagation ‚úÖ
  - Python: Flask/FastAPI middleware ‚úÖ
  - Go: net/http middleware with public getters (ServiceName, InstanceID) ‚úÖ
  - Rust: Axum tower::Layer ‚úÖ
- ‚úÖ **Cross-Language Demo** (`examples/distributed/`)
  - TypeScript ‚Üí Python ‚Üí Go ‚Üí Rust HTTP chain ‚úÖ
  - Bash orchestration (no Docker required) ‚úÖ
  - Integration smoke test script with header validation ‚úÖ
- ‚úÖ **Demo Applications** (complete on ports 6001-6004)
- ‚úÖ **Integrated into `./raceway-dev` menu** (option 11)

**Next Steps:**
1. ‚úÖ ~~Complete all SDK trace context modules~~ DONE!
2. ‚úÖ ~~Implement middleware integration~~ DONE!
3. ‚úÖ ~~Build `examples/distributed/` smoke test~~ DONE!
4. **Phase 2: Engine changes for cross-service graph edges** ‚Üê Next milestone
5. Cross-language integration testing with full graph support

**Key Achievement:** All 4 SDKs now have identical trace context implementations with comprehensive test coverage (95 total tests across all SDKs)!

**Phase 1.5 Goal:** Validate SDK propagation works end-to-end BEFORE requiring backend changes (Phase 2). This allows parallel development and early validation.

### Objectives
- Implement the shared trace-context layer across SDKs using the decisions from Phase‚ÄØ0.
- Ensure every SDK can inject and extract distributed tracing metadata over HTTP calls.
- Provide sample middleware/wrappers so framework integration is straightforward.

### Deliverables
- Document header usage (`traceparent`, `tracestate`, `raceway-clock`) for all SDKs.
- Shared serialization/deserialization libraries in each SDK (TypeScript, Python, Go, Rust).
- Middleware/reference implementations for the primary frameworks per language.
- Automated tests or linters verifying builds (TypeScript `tsc`, Rust `cargo fmt`, Go/Python format checks).
- Sample ‚ÄúService A ‚Üí Service B‚Äù demos (pending cross-language chain).

### Work Breakdown

1. **Trace Context Modules**
   - Define language-specific structures mirroring the `raceway-clock v1` schema.
   - Implement helper APIs:
     - `TraceContext::from_headers(headers) -> Result<Option<Context>>`
     - `TraceContext::to_headers(&self) -> HeaderMap`
     - `TraceContext::merge(&mut self, incoming: &Context)`
   - Include validation (UUID formats, version prefixes) and error reporting.

2. **Outbound Propagation**
   - **TypeScript**
     - Add fetch/axios interceptors that pull context from AsyncLocalStorage and set headers.
     - Extend Express middleware to initialize context and wrap downstream `RacewayClient`.
   - **Python**
     - Provide `requests` and `httpx` session adapters that inject headers.
     - Flask/FastAPI: route middleware seeds context, utilities for background tasks.
   - **Go**
     - Implement `http.RoundTripper` wrapper plus helper for `http.Client`.
     - Add context helpers (`context.Context`) storing trace/span ids.
   - **Rust**
     - Provide `tower::Layer` for axum/reqwest.
     - Add macros or helpers to wrap `reqwest::Client` requests.

3. **Inbound Extraction**
   - Extend existing middleware for Express, Flask/FastAPI, Gin, and Axum to:
     - Parse headers via `TraceContext::from_headers`.
     - Merge with local context (element-wise max of vector clock).
     - Generate new span ids when missing, mark context as ‚Äúdistributed=false‚Äù if no headers.

4. **SDK Surface Area Updates**
   - Add ability to set service name/instance id (if not already) for clock keys.
   - Document new config knobs (e.g., `propagation_enabled`, `propagation_headers_debug`).
   - Provide logging hooks to detect propagation issues (missing header, parse failure).

5. **Demo Applications**
   - For each SDK, create a minimal two-service example:
     - Service A receives a request, calls Service B using the SDK client, both emit events, verify they share the same distributed trace id/span structure.
   - Consolidate demos under `examples/distributed/<language>-chain`.

6. **Phase 1.5: Integration Smoke Test**
   - Build cross-language demo: TypeScript ‚Üí Python ‚Üí Go ‚Üí Rust HTTP chain
   - Validate SDK propagation WITHOUT requiring Phase 2 backend changes
   - Location: `examples/distributed/` with docker-compose orchestration

   **What This Validates:**
   - ‚úÖ Headers propagate correctly across all 4 SDKs
   - ‚úÖ Vector clocks accumulate components from all services
   - ‚úÖ Events from all services share the same `trace_id`
   - ‚úÖ Service metadata correctly identifies each service
   - ‚úÖ Middleware integration works in all frameworks (Express, Flask, net/http, Axum)

   **Known Limitations (Acceptable without Phase 2):**
   - ‚ö†Ô∏è Graph shows 4 disconnected sub-graphs (no cross-service edges yet)
   - ‚ö†Ô∏è Critical path doesn't span services (calculated per-service only)
   - ‚ö†Ô∏è No cross-service race detection (within-service races still work)
   - ‚ö†Ô∏è No distributed span hierarchy (span linkage is local to each service)

   **Test Script:** `examples/distributed/test.sh`
   - Starts all services via docker-compose
   - Makes request through full chain
   - Validates headers, vector clocks, and event grouping
   - Reports success with clear note about Phase 2 requirements

### Testing Plan (Phase 1)

| Layer            | Tests                                                                                                     |
|------------------|-----------------------------------------------------------------------------------------------------------|
| Serialization    | Unit tests for encode/decode of `raceway-clock` (valid data, version mismatch, invalid base64). ‚úÖ DONE (95 tests)  |
| Header handling  | Unit tests ensuring `traceparent`/`tracestate` round-trip; verify vendor header appended without clobbering existing state. ‚úÖ DONE |
| Middleware       | Integration tests spinning up local HTTP servers asserting that contexts propagate end-to-end (Service A -> Service B). ‚è≥ NEXT |
| Merge semantics  | Property tests (where available) checking element-wise max behavior and TTL trimming. ‚úÖ DONE (in unit tests) |
| Regression       | Ensure single-process traces still behave identically when propagation disabled. ‚è≥ After middleware |
| Smoke Test       | `examples/distributed/test.sh` - Full TS‚ÜíPy‚ÜíGo‚ÜíRust chain validates header propagation and event grouping. ‚è≥ Phase 1.5 |

### Exit Criteria

**Phase 1 (Trace Context Modules) - ‚úÖ COMPLETE**
- ‚úÖ All 4 SDKs have trace context serialization/parsing (95 tests passing)
- ‚úÖ Cross-SDK header compatibility validated via unit tests
- ‚úÖ Vector clock merge semantics tested

**Phase 1.5 (SDK Propagation) - ‚úÖ COMPLETE**
- ‚úÖ All maintained SDKs propagate headers on outbound HTTP requests and consume them on inbound requests
- ‚úÖ Middleware integration complete for Express, Flask, net/http, Axum
- ‚úÖ `examples/distributed/` demo runs successfully (bash orchestration)
- ‚úÖ Smoke test validates: header propagation, vector clock accumulation, trace_id consistency
- ‚úÖ Demo integrated into `./raceway-dev` menu (option 11)
- ‚úÖ Go SDK enhanced with public getters for ServiceName() and InstanceID()

**Phase 1.5 Success Criteria - ‚úÖ ACHIEVED:**
```bash
$ cd examples/distributed && ./test.sh
Starting TypeScript service...
Starting Python service...
Starting Go service...
Starting Rust service...

Waiting for TypeScript (port 6001)... ‚úì
Waiting for Python (port 6002)... ‚úì
Waiting for Go (port 6003)... ‚úì
Waiting for Rust (port 6004)... ‚úì

‚úì All services are healthy!

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Linear Pattern Test: TS ‚Üí Python ‚Üí Go ‚Üí Rust
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

W3C traceparent headers:
  ‚óã TypeScript (entry point - no incoming headers)
  ‚úì Python received traceparent
  ‚úì Go received traceparent
  ‚úì Rust received traceparent

Raceway vector clock headers:
  ‚óã TypeScript (entry point - no incoming headers)
  ‚úì Python received raceway-clock
  ‚úì Go received raceway-clock
  ‚úì Rust received raceway-clock

‚úì Linear pattern test PASSED
  - Headers propagated across all 4 services
  - All services share the same trace_id

‚ö†Ô∏è  Note: Graph will show 4 disconnected sub-graphs
   (cross-service edges require Phase 2)

‚úì All tests completed!
```

---

## Phase 2 ‚Äì Engine & Storage Enhancements

### Status: ‚úÖ Phase 2 Core Complete!

**Current Progress:**

**Phase 2.1: SDK Distributed Metadata - ‚úÖ COMPLETE!**
- ‚úÖ **All SDKs send distributed metadata unconditionally** (not gated by distributed flag)
  - TypeScript: Always sets instance_id, distributed_span_id, upstream_span_id when context exists ‚úÖ
  - Python: Always sets distributed metadata when context exists ‚úÖ
  - Go: Always sets distributed metadata when context exists ‚úÖ
  - Rust: Always sets distributed metadata when context exists ‚úÖ
- ‚úÖ **Entry-point services create distributed spans** (fixed bug where they previously didn't)

**Phase 2.2: Storage - ‚úÖ COMPLETE!**
- ‚úÖ **PostgreSQL tables implemented and working:**
  - `distributed_spans(trace_id, span_id, service, instance, first_event)` ‚úÖ
  - `distributed_edges(from_span, to_span, edge_type, metadata)` ‚úÖ
  - Indexes on `(trace_id)`, `(span_id)`, `(from_span, to_span)` ‚úÖ
- ‚úÖ **Storage trait extended:**
  - `get_distributed_spans(trace_id) -> Result<Vec<DistributedSpan>>` ‚úÖ
  - `get_distributed_span(span_id) -> Result<Option<DistributedSpan>>` ‚úÖ
  - `get_distributed_edges(trace_id) -> Result<Vec<DistributedEdge>>` ‚úÖ
  - `upsert_distributed_span()` and `upsert_distributed_edge()` ‚úÖ

**Phase 2.3: Backend Merging - ‚úÖ COMPLETE!**
- ‚úÖ **Recursive BFS implementation in `core/src/analysis.rs:316-447`:**
  - Uses BFS queue to recursively discover all connected spans
  - Follows edges through arbitrary chain lengths (tested with 4-service chain)
  - Groups spans by trace_id to minimize queries
  - Filters events by distributed_span_id
  - Sorts merged events chronologically
  - Logs: "Merged trace X: Y events from Z spans across N traces"
- ‚úÖ **4-Service Chain Validated:**
  - TypeScript ‚Üí Python ‚Üí Go ‚Üí Rust successfully merged
  - 13 events from 5 distributed spans across 1 trace
  - All services present in chronological event timeline

**Phase 2.4: Event Ingestion - ‚úÖ COMPLETE!**
- ‚úÖ **Event metadata includes distributed fields:**
  - `instance_id: Option<String>`
  - `distributed_span_id: Option<String>`
  - `upstream_span_id: Option<String>`
- ‚úÖ **Spans and edges automatically created during ingestion**
- ‚úÖ **HttpCall/GrpcCall events create distributed edges**

**What's Working:**
```bash
$ cd examples/distributed && ./patterns/full-chain.sh

Making single request: TypeScript ‚Üí Python ‚Üí Go ‚Üí Rust...

üìä MERGED DISTRIBUTED TRACE ANALYSIS
======================================================================
Trace ID: 332e9289-da37-4670-b75d-e38f517e8909
Total Events: 13

Events by Service:
  ‚Ä¢ go-service: 3 events
  ‚Ä¢ python-service: 4 events
  ‚Ä¢ rust-service: 3 events
  ‚Ä¢ typescript-service: 3 events

‚úÖ SUCCESS: All 4 services present in merged trace!
‚úÖ Recursive BFS distributed tracing working end-to-end!

This proves the system scales to arbitrary chain lengths!
```

**Pending (Phase 2.5):**
- ‚è≥ Vector clock merge logic (element-wise max) - currently accumulates but not merged in graph
- ‚è≥ CausalGraph updates for cross-service edges (external edge linking)
- ‚è≥ Config toggles for distributed tracing (currently always enabled)
- ‚è≥ Unit tests for graph merge logic
- ‚è≥ Property tests for vector clock semantics

### Original Requirements

### Engine

- ‚úÖ Modify `Event` ingestion to accept optional remote parent references (service, span ID, vector clock).
- ‚è≥ Update `CausalGraph`:
  - ‚è≥ Allow multiple root nodes per logical trace, merging by trace id.
  - ‚è≥ Add support for "external edge" linking events from different services.
  - ‚è≥ Handle missing parents gracefully (e.g., dropped headers) with warning edges.

### Vector Clocks

- ‚úÖ Extend from per-trace `DashMap<Uuid, u64>` to per-service or per-span component.
- ‚úÖ Introduce identifier scheme (service name + instance id) to avoid collisions.
- ‚è≥ Keep clocks bounded (evict old components).

### Storage

- ‚úÖ Postgres migrations:
  - ‚úÖ New table for distributed edges (`distributed_edges`).
  - ‚úÖ Indexes for `trace_id` + `service_name`.
  - Optional materialized view for service dependency map.
- ‚úÖ Memory backend:
  - ‚úÖ Mirror distributed edges in-memory (e.g., `DashMap<(Uuid, Uuid), DistributedEdge>`).

### Routing/Config

- ‚è≥ Add server config toggles (`distributed_tracing.enabled`, default false).
- ‚è≥ Gracefully reject distributed headers when disabled.

### Testing

- ‚è≥ Unit tests for graph merge logic.
- ‚è≥ Property tests ensuring vector clock happens-before remains correct after merges.
- ‚úÖ Storage tests verifying migrations and retrieval of cross-service data.
- ‚úÖ Engine integration test: ingest events from two "services" and ensure graph connectivity.

**Exit criteria:** ‚úÖ **Core engine can accept distributed events and construct a coherent graph; persisted structures support fetching.** ACHIEVED! Recursive BFS proves arbitrary-length chains work end-to-end.

---

## Phase 3 ‚Äì UI & Analytics

### Terminal UI

- Service-aware trace list (show service count).
- Graph view grouping nodes by service with color coding.
- Critical path spanning services.
- Alert when edges missing (header loss).

### Web UI

- Update service dependency graph to use distributed edges.
- Timeline overlay per service.
- Filters for service/operation.

### API

- `/api/traces/:id` responses include distributed edges and service metadata.
- New endpoints (optional) for service-level metrics (e.g., `/api/services/:name/dependencies`).

### Testing

- Snapshot tests for API payloads.
- Cypress/Playwright flows verifying multi-service trace visualization.
- TUI golden tests (ratatui) verifying layout with distributed data.

**Exit criteria:** Users can inspect multi-service traces end-to-end in both UIs.

---

## Phase 4 ‚Äì Advanced Propagation & Resilience

- gRPC interceptors for all SDKs.
- Message queue/staged propagation (Kafka headers, etc.).
- Fallback clocks when headers missing (generate synthetic spans, mark as incomplete).
- Replay support: ability to rehydrate distributed traces from storage into engine.
- Telemetry & observability: metrics for header adoption, drop rate, clock conflict warnings.

### Tests

- End-to-end scenario: service A ‚Üí gRPC ‚Üí service B ‚Üí Kafka ‚Üí service C.
- Chaos tests: randomly drop headers to ensure system degrades gracefully.
- Load tests to measure impact on ingestion and storage.

---

## Testing Plan Summary

| Area                  | Tests                                                                                   |
|-----------------------|-----------------------------------------------------------------------------------------|
| SDKs                  | Unit tests for serialization, integration tests with mock servers, contract tests       |
| Engine/Graph          | Unit & property tests on clock merge, integration test ingesting multi-service events    |
| Storage               | Migration tests, round-trip tests for distributed edges                                 |
| APIs                  | Regression tests verifying JSON structure for distributed traces                        |
| TUI/Web               | Snapshot/golden tests for distributed visualizations                                    |
| End-to-End            | Multi-service demo app executed in CI (docker-compose) verifying trace continuity       |
| Resilience            | Chaos-style tests dropping headers, clock skew simulation                               |

---

## Milestones & Deliverables

1. ‚úÖ **M0 (Design sign-off)** ‚Äì approved spec, storage plan, SDK API changes outlined. **COMPLETE**
2. ‚úÖ **M1 (HTTP propagation)** ‚Äì SDKs send/receive headers; basic engine support behind a feature flag. **COMPLETE**
3. ‚úÖ **M2 (Engine/storage GA)** ‚Äì distributed traces persisted; API surfaces cross-service data; tests passing. **CORE COMPLETE**
   - ‚úÖ All 4 SDKs sending distributed metadata
   - ‚úÖ Storage tables and queries working
   - ‚úÖ Recursive BFS merging arbitrary-length chains
   - ‚úÖ 4-service chain validated end-to-end
   - ‚è≥ Remaining: CausalGraph updates, config toggles, comprehensive testing
4. ‚è≥ **M3 (UI support)** ‚Äì TUI/web show multi-service traces; feature flag default-on for beta users.
5. ‚è≥ **M4 (Full release)** ‚Äì gRPC/queues supported; resilience tooling; documentation and sample apps updated.

---

## Documentation & Rollout Tasks

- Update README and docs with configuration and usage instructions.
- Publish sample multi-service demo (docker-compose).
- Release notes per milestone with migration guidance.
- Provide observability guidance (verify context headers in network probes).
